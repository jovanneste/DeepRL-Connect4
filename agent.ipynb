{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116b2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle_environments==0.1.6 in /opt/anaconda3/lib/python3.8/site-packages (0.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle_environments==0.1.6) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle_environments==0.1.6) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle_environments==0.1.6) (0.17.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle_environments==0.1.6) (65.6.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema>=3.0.1->kaggle_environments==0.1.6) (1.15.0)\n",
      "['random', 'negamax']\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle_environments==0.1.6\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6215be61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 6, 20)             160       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 6, 20)             420       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 6, 7)              147       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 727\n",
      "Trainable params: 727\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the DQN\n",
    "\n",
    "action_space = 7\n",
    "input_space = (6,7)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(20, activation=tf.nn.relu, input_shape=input_space),  \n",
    "  tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(action_space)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4e0e75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def epsilonDecision(epsilon):\n",
    "    return random.choices(['model','random'], weights = [1 - epsilon, epsilon])[0]\n",
    "\n",
    "\n",
    "def chooseAction(model, observation, epsilon):\n",
    "    action_decision = epsilonDecision(epsilon)\n",
    "    observation = np.array([observation])\n",
    "    preds = model.predict(observation)\n",
    "    weights = tf.nn.softmax(preds).numpy()[0]\n",
    "    \n",
    "    if action_decision == 'model':\n",
    "        action = np.argmax(weights)\n",
    "    if action_decision == 'random':\n",
    "        action = random.randint(0,6)\n",
    "        \n",
    "    return int(action), weights\n",
    "\n",
    "\n",
    "def checkValid(obs, action):\n",
    "    valid_actions = set([0,1,2,3,4,5,6])\n",
    "    try:\n",
    "        if obs[0,action] != 0:\n",
    "            valid_actions = valid_actions - set([action])\n",
    "            action = random.choice(list(valid_actions))\n",
    "    except:\n",
    "        action = random.choice(list(valid_actions))\n",
    "    return action\n",
    "\n",
    "\n",
    "def getReward(winner, state):\n",
    "    if not state:\n",
    "        reward = 0\n",
    "    if state: \n",
    "        if winner == 1:\n",
    "            reward = 50\n",
    "        if winner == -1:\n",
    "            reward = -50\n",
    "        if winner == 0:\n",
    "            reward = -50\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "96eaf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    def __init__(self):\n",
    "        self.clear() \n",
    "        \n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def store_experience(self, new_obs, new_act, new_reward):\n",
    "        self.observations.append(new_obs)\n",
    "        self.actions.append(new_act)\n",
    "        self.rewards.append(new_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d189f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(target_q_values, predicted_q_values):\n",
    "    return tf.reduce_mean(tf.square(target_q_values - predicted_q_values))\n",
    "\n",
    "\n",
    "def update_q_network(states, actions, rewards, gamma=0.99):\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(states)\n",
    "        selected_action_indices = tf.range(0, tf.shape(actions)[0]) * tf.shape(q_values)[1] + actions\n",
    "        selected_q_values = tf.gather(tf.reshape(q_values, [-1]), selected_action_indices)\n",
    "        target_q_values = rewards\n",
    "        loss = compute_loss(target_q_values, selected_q_values)\n",
    "   \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8403b8c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_agent(n):\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    replay = Experience()\n",
    "    epsilon = 1\n",
    "    epsilon_rate = 0.995\n",
    "    win_track = []\n",
    "\n",
    "    for episode in tqdm.tqdm(range(n)):\n",
    "        trainer = env.train([None, 'random'])\n",
    "        state = np.array(trainer.reset()['board']).reshape(6,7)\n",
    "        replay.clear()\n",
    "        epsilon = epsilon*epsilon_rate\n",
    "\n",
    "        done=False\n",
    "        while not done:\n",
    "            action, w = chooseAction(model, state, epsilon)\n",
    "\n",
    "            while True:\n",
    "                t_action = action\n",
    "                action = checkValid(state, t_action)\n",
    "                if t_action==action:\n",
    "                    break\n",
    "\n",
    "            new_state, winner, done, info = trainer.step(action)\n",
    "            state = np.array(new_state['board']).reshape(6,7)\n",
    "            reward = getReward(winner, done)\n",
    "            replay.store_experience(state, action, reward)\n",
    "            \n",
    "            if done:\n",
    "                win_track.append(winner)\n",
    "                update_q_network(np.asarray(replay.observations), replay.actions, replay.rewards)\n",
    "                break\n",
    "        \n",
    "    print(\"Training against random agent finished\")\n",
    "    return (np.count_nonzero(np.asarray(win_track)==1)/n)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6fef6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_agent(observation, configuration):\n",
    "    action = chooseAction(model, np.asarray(observation.board).reshape(6,7), 1)\n",
    "    return action[0]\n",
    "\n",
    "def test(observation, configuration):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4eff5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 1 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 1 | 0 | 2 | 0 | 2 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 1 | 1 | 0 | 1 | 1 | 1 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2 | 2 | 2 | 2 | 1 | 2 | 1 |\n",
      "+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.run([q_agent, \"random\"])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700be454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
